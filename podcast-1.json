{"podcast_details": {"podcast_title": "POLITICO Tech", "episode_title": "One tech's bold idea: AI is the new atomic energy. Nationalize it.", "episode_image": "https://image.simplecastcdn.com/images/be2d3049-4f3b-4332-9bec-df8b43b108c8/e908c093-19be-4b2d-b16e-c06cadc736e6/3000x3000/tech-for-apple-only-3000x3000-audio.jpg?aid=rss_feed", "episode_transcript": " Hey, welcome back to Politico Tech. It's Monday, August 21st. I'm your host, Stephen Overland. Today's guest has an idea so bold, he quickly put a disclaimer on it. Frankly, it's a bad idea. It's just all the other choices are so much worse. That's Charles Jennings. Since the 90s, he's been the CEO of a string of West Coast software companies. And last was an artificial intelligence firm called Neural Eye that developed facial recognition and computer vision technology. But now in retirement, Charles has taken a more cautious, some might even say fearful, view of AI, which led to his idea to nationalize artificial intelligence. I love this show. It's a little bit radical, but it's the kind of conversations I think a lot of us want to be having, a lot of us should be having when we talk about emerging technologies. So on today's show, you'll hear his concerns about the potential for the most advanced AI systems to spiral out of control and even threaten our humanity. In this article you've written for Politico Magazine, you have this bold idea, which is to nationalize artificial intelligence. How would that actually work? A, my article is meant to be provocative. My chief goal is to get everyone thinking. B, I think we need to nationalize parts of AI, as I made clear in the heart of the article, the leading edge of it, the most advanced. It would be impractical to nationalize all of AI as if it were a railroad. That's not what I'm saying. It's that we need to have government leadership of the cutting edge elements of this society. And I mean of this technology. And so it's very important that we either A, induce the big tech companies who control AI, the world leaders in it, you know, the names, Amazon, Microsoft, et cetera. We need to get them to come in to a level of collaboration that has not happened yet between government and tech. Or we need to nationalize the tech. Those are the two choices, I think, for them. In this context, how are you defining cutting edge exactly? Because to me, AI is one of these technologies that is evolving so quickly, it sort of seems like cutting edge is kind of a moving target. Absolutely. There are a number of major, highly advanced AI models in the world today. And there will be more and more every day in the future. I'm not interested or suggesting that we nationalize every iteration, every use, every application of these models. It's the models themselves, the core capability to autonomously learn and to operate autonomously that needs to get special attention by government. And at the end of the day, we only have two choices. I call these this type of technology the equivalent of the nuclear codes of AI. This stuff is really powerful. And we only have two choices. Either the big tech guys run it or we, the people, the citizens do through the government. It's not going to be easy. Government's not really equipped to do that today. I'm not saying Congress should regulate it. I don't think Congress is remotely capable of keeping up with AI. We need something new. I call it the Humane AI Commission. Humane AI Commission, okay. What would that look like? Because I'm wondering what kind of government structure would be equipped to manage this cutting edge AI. So luckily we have sort of a precedent. It's called the Atomic Energy Commission. People are now more familiar with it because of the film Oppenheimer. It was created by Truman in 1947. It consisted initially of five citizens. And against all odds, I mean, they had their problems as the film makes points out, but they kept the world safe from nuclear war, nuclear weapons for decades. And the way they did it was they were not political or overtly political. And eventually what happened by the mid-50s, it became controlled by experts who had no financial interest in nuclear weapons. And the AEC, Atomic Energy Commission, actually owned the nuclear reactors that were used by the military for bombs, by Westinghouse for nuclear energy and so forth. My proposal for the Humane AI Commission is that we set up a new body largely separated from government with the best AI experts we can find who are also not financially conflicted. And A and B, I think it needs to be a diverse group. One of the problems we have, even for the more enlightened tech executives, is that they're all male and they all are part of a very narrow tech culture. If AI is going to provide the big dividend that I can, that's the positive side, the big AI benefit, it's going to have to be controlled by people with a greater and more diverse group of interests, life experiences, and it's got to be more humane than big tech wants it to be. There are going to be people who read this article or listen to our conversation here and think if you nationalize cutting edge AI, that's essentially the same thing as killing it off in the US and setting the US at a disadvantage in this global AI race. Is there validity to that argument in your view or not? No. In fact, there was an open letter of 700 AI scientists recently, sponsored by the Future of Life Institute, a group. I have signed some of their previous petitions myself, but I did not sign this one because if we paused AI, they were calling for a pause in AI development. Major AI programs should be stopped for six months, so we can figure out what's happening. If that's actually even possible, which I doubt, the big beneficiary would be China. We are in a horse race with China, and China has a lot of AI investment and capability. They have the best K-12 childhood AI education in the world. They have a whole city, Hangzhou, which is being run by an AI today very effectively, but they are using AI to spy on Uyghurs. They have a huge AI militarization program. We have no choice but to keep moving ahead aggressively, but are we willing to put all of the control into the hands of six or seven big tech CEOs, all male, all very much from one culture, and frankly, guys who use AI as a commercial weapon. Famously now, when Microsoft added chat GBT to Bing, Satya Nadella, the CEO of Microsoft, did an interview with Verge Magazine where he said, quote, I just wanted to make Google dance, which is to me such a Wild West image. Is that what they're thinking? And Nadella is one of the good guys, probably the best. But they're not the right people to be managing technology this powerful. So we have to find a way, just as we did with the Atomic Energy Commission, nobody had figured it out in advance, but we have to get the government involved in some way, or we just say, okay, tech guys, we're going to cross our fingers and hope you figure this out. I mean, there's certainly a drive for innovation that you have to question what guardrails are on it and what bigger picture societal questions are being asked when engineers are constantly sort of pursuing the next iteration, the next evolution of technology like AI. You mentioned you were the CEO of a company called Neural Eye, which was an AI company. How would you have reacted if sort of the government had swooped in here and said, okay, this technology is dangerous, we're going to need to take it over now? I'd have lobbied against it. I don't think I would have even conceived of nationalization component of our AI policy as even being possible a year ago. But this is an extraordinary technology. This is not just another kind of software. This is what I've experienced in the AI labs. This technology, I believe, has emergent properties. That means it can pull a rabbit out of its hat and do things that you didn't know were possible. It is growing so fast. It's about to have this huge new acceleration from quantum computing, which is coming sooner than most people think. It's racing ahead out of control now. I think that's why Jeff Hinton, the inventor of deep learning, the father of AI, probably the smartest person in the world with respect to AI, recently resigned from Google because of his concerns about what's happening with AI. Is your concern or what kind of nerves you most here that idea of AI kind of getting away from us and moving beyond us being, I guess, humans? That seems to be kind of a recurring concern in what you wrote about as well, which is this idea that AI will evolve beyond our own understanding of it and then we're really up a creek. That's correct. It's not the only risk. There's the risk of bad actors using AI against us. Think of the malware hackers on steroids. There are nation states, rogue states, North Korea, Iran, others that might become much more powerful and lethal through the use of AI. The genie is out of the bottle. I'm not saying we're going to go backwards or that we should. As I say in the article, we have no choice but to go forward. The international governing structures are much weaker than they were in Truman's time. The UN is very ineffective. Some of my colleagues advocate a CERN type program, an international governing body. I don't think that'll work right now. I don't think we have any choice but as a nation, the United States of America, to confront the challenge of AI and the issue of big tech exclusive control. You're based on the West Coast. You're not here in Washington. How are you anticipating this will be received among your tech colleagues there on the West Coast? I've had to tell people, you may need to disconnect me from LinkedIn and everything else. I don't imagine it's going to be a popular idea. Frankly, were I not 75 years old and retired, maybe I wouldn't be saying it. I don't know. Right now, I wrote a book on AI, did a lot of research, talked to a lot of people around the world about AI. I just had to be honest about where I see it going and what the best path to manage it would be. Government nationalization would be very difficult and painful. Frankly, it's a bad idea. It's just all the other choices are so much worse. But we've got to do something. I do just follow up questions for you on what you've said there. You mentioned being 75, being retired, which I wonder to what extent is your perspective here shaped by having been through a career, having worked in tech, and having the capacity to reflect now that you're no longer in the thick of it and feeling that pressure to innovate yourself. I don't know that it's the lack of pressure to innovate, but I must say that I started my first internet company in 1993. I've started a number of tech companies. I've seen our industry, particularly with respect to privacy, which has always been a key issue of mine. The rise of social media, the misinformation, disinformation capabilities that have developed because of technology, all those have been an increasing concern to me. I am at the stage now where what's really important to me is that we preserve our core human values. I think getting AI managed properly is part of that. The other side of it, which I do not speak about in the article, I do in my book a bit, is we're going to need to think about what it means to be human when we're no longer the smartest species in the room all the time. What is going to be left? I work with the University of Portland as a consultant trying to figure out what the right curriculum is for today's college students when a lot of the traditional careers, the white collar careers are going to be taken over by AI. We need to double down on understanding what it means to be human. We also need to understand there's a powerful new technology in our midst, not unlike electricity in the turn of the 20th century, changed everything. Not unlike nuclear energy in the 1940s. We were able to get our hands around both of those somewhat successfully. We're still here, but this one is another serious challenge. When you talk to your contemporaries about this, how often is this the topic of conversation, and do you think that there's agreement with you on these concerns about AI, on this sort of human identity crisis? You know all of it. There is no question that there's great concern out there. In my circle of colleagues and friends, everyone knows of my AI focus and they're quite anxious to talk about it. On the other hand, I don't know that I've run into anybody who agrees that we should nationalize certain components of AI. That's a radical idea. I get it. It's not something that most people even want to think about. But I have to say, after wrapping my head against this wall for a number of years, I spent three years on AI policy consulting with the Atlantic Council, trying to solve the Rubik's Cube of AI policy. What is the right thing to do? And honestly, this is the only path I can see that gives us a chance of turning this into something really positive for humanity and protecting against its biggest threats. Well, Charles, I appreciate you being on Politico Tech. Hopefully, you're not driven into witness protection by the folks out there in Silicon Valley. But I'm glad you brought this idea to us today. Well, okay. Again, my main intent is to get everyone thinking about it more, talking about it, because it really is an important issue. Absolutely. Thank you, Stephen. Take care. Okay, bye. You can find Charles's full article on Politico.com. Here are two events I'll be watching today. Senate Commerce Chair Maria Cantwell is holding an AI forum with reps from Amazon and Microsoft in her native Washington state. And in New Hampshire, Senator Maggie Hassan is convening a field hearing on the cyber threats facing K-12 schools. That's all for Politico Tech today. Should the federal government hold the keys to AI? Give us your take at techpodcast at politico.com. And for more tech news, subscribe to our newsletters, Digital Future Daily and Morning Tech. Music in today's show comes from the mysterious Breakmaster Cylinder. Our senior producer is Annie Reese. Our editors are Steve Heuser and Louisa Savage. I'm Stephen Overly. Let's meet back here tomorrow."}, "podcast_summary": "In this episode of Politico Tech, host Stephen Overland interviews Charles Jennings, the CEO of Neural Eye and a former AI company executive. Jennings proposes the nationalization of artificial intelligence (AI) to address concerns about the potential dangers and threats posed by advanced AI systems. He argues that government leadership is needed to manage the cutting-edge elements of AI, especially as it continues to evolve rapidly. Jennings suggests inducing collaboration between big tech companies and the government or nationalizing the technology if necessary. He also advocates for the establishment of a Humane AI Commission, similar to the Atomic Energy Commission, to oversee AI development with experts who are not financially conflicted. Jennings highlights the risks associated with AI, including misuse by bad actors and the need for international governing structures. He concludes by emphasizing the importance of managing AI properly to preserve our core human values and addresses the challenges AI poses to human identity and traditional careers. The episode concludes with a call for more discussion and consideration of the issue. Overall, Jennings presents a thought-provoking perspective on the complex and multifaceted issues surrounding AI and the need for responsible management.", "podcast_guest": "Charles Jennings is the esteemed CEO and founder of Neural Eye, a leading technology company specializing in artificial intelligence and computer vision solutions. With over 15 years of experience in the tech industry, Charles has established himself as a visionary leader and expert in the field. Prior to founding Neural Eye, he held senior roles at prominent tech firms, where he led teams in groundbreaking projects, including the development of advanced computer vision algorithms. Charles is highly regarded for his ability to merge cutting-edge technology with practical applications, making him a sought-after thought leader and innovator in the industry. His deep knowledge and experience in AI, coupled with a proven track record of driving successful ventures, make Charles Jennings an ideal guest for any podcast exploring the transformative potential of artificial intelligence.", "podcast_highlights": "1. Charles Jennings proposes the nationalization of artificial intelligence, specifically the cutting-edge elements, to avoid it spiraling out of control and becoming a threat to humanity.\n2. To achieve this, Jennings suggests inducing collaboration between government and big tech companies or outright nationalizing the technology.\n3. He defines cutting-edge AI as the advanced AI models that have the capability to autonomously learn and operate independently.\n4. Jennings proposes the creation of a Humane AI Commission, similar to the Atomic Energy Commission, consisting of diverse AI experts who are not financially conflicted.\n5. While some may argue that nationalizing cutting-edge AI would put the US at a disadvantage globally, Jennings believes that the risk of not having government involvement outweighs the benefits of leaving it solely in the hands of big tech CEOs.\n\nKey moment: \"We have no choice but to keep moving ahead aggressively, but are we willing to put all of the control into the hands of six or seven big tech CEOs, all male, all very much from one culture, and frankly, guys who use AI as a commercial weapon.\"\n"}
